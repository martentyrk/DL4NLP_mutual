# -*- coding: utf-8 -*-
"""BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oMeAdmecvCuV9Tup-rR2V_T2bEaaSxbj
"""

import argparse
import logging
import json
import os
import random
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm
import torch.optim as optim
import torch.utils.data as data
from torch.utils.data import DataLoader, Dataset
from transformers import AutoModel, AutoTokenizer
from transformers import BertTokenizer, BertModel
import pytorch_lightning as pl
from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint


def load_model(model_name, num_classes, freeze_lm=True):
    """
    This function loads the a pretrained model and add a classifier layer on top
    Inputs:
        model_name - name of the pretrained model
        in_features: input dimension of classifier layer
        num_classes: number of classes which is the dimension of output of classifier layer
        freeze_lm: boolean parameter indicating if to freeze weights of pretrained model
    """
    ## Load pretrained model
    model = BertModel.from_pretrained("bert-base-uncased")

    ## freeze all weights in LM to reduce computational complex
    if freeze_lm:
        for name, param in model.named_parameters():
            param.requires_grad = False

    ## Define a classifier layer and add it to the LM
    output_dimension = model.config.hidden_size
    linear_layer = nn.Linear(output_dimension, num_classes)
    nn.init.normal_(linear_layer.weight, 0, 0.01)
    
    model = nn.Sequential(model, linear_layer)

    return model

def create_dataset(data_path, model_name):
    """
    This function creates dataset for dataloader
    Inputs:
        data_path: path to dataset
        model_name - name of the pretrained model
    """
    ## Load tokenizer of chosen model
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

    ## Load data
    with open(data_path, "r") as file:
      data = file.read()
    
    data = json.loads(data)
    ## Get diagolue, choices, and answers
    contexts = [sample['article'] for sample in [data]]
    choices = [sample['options'] for sample in [data]]
    map_to_int = {'A': 0, 'B': 1, 'C': 2, 'D': 3}
    labels = [map_to_int[sample['answers']] for sample in [data]]
    ## Create data by concatenating context with each choice
    X = [tokenizer([context]*4,
                   choice,
                   return_tensors="pt",
                   padding=True) for context, choice in zip(contexts, choices)]
    y = [torch.tensor(label).unsqueeze(0) for label in labels]
    return X,y


class MultuDataset(Dataset):
    """
    Custom dataset class
    """
    def __init__(self, data, labels):
        self.data = data
        self.labels = labels

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx], self.labels[idx]

class Multu_Module(pl.LightningModule):
    """
    Torch lightning training pipeline
    """
    def __init__(self, args):
          """
          Inputs:
              args - user defined arguments
          """
          super().__init__()
          self.model = load_model(args.model_name,
                                  args.num_classes,
                                  args.freeze_lm)
          self.loss_module = nn.CrossEntropyLoss()
          self.optimizer_name = args.optimizer

    def forward(self, instance):
        return self.model(instance)


    #TODO: Add hparameter for learning rate
    def configure_optimizers(self):
        if self.optimizer_name == "Adam":
            optimizer = optim.AdamW(
                self.parameters(), 1e-3)
        elif self.optimizer_name == "SGD":
            optimizer = optim.SGD(self.parameters(), 1e-3)
        else:
            assert False, f"Unknown optimizer: \"{self.optimizer_name}\""
            
            
        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, total_steps=10)
        return [optimizer], [scheduler]
        

    def training_step(self, batch, batch_idx):
        sequence, labels = batch
        preds = self.model(sequence)
        loss = self.loss_module(preds, labels)
        acc = (preds.argmax(dim=-1) == labels.argmax(dim=-1)).float().mean()
        self.log('train_acc', acc, on_step=False, on_epoch=True)
        self.log('train_loss', loss)
        return loss

    def validation_step(self, batch, batch_idx):
        sequence, labels = batch
        preds = self.model(sequence).argmax(dim=-1)
        acc = (preds.argmax(dim=-1) == labels.argmax(dim=-1)).float().mean()
        self.log('val_acc', acc)

    def test_step(self, batch, batch_idx):
        sequence, labels = batch
        preds = self.model(sequence).argmax(dim=-1)
        acc = (preds.argmax(dim=-1) == labels.argmax(dim=-1)).float().mean()
        self.log('test_acc', acc)

def fine_tune(args):
    """
    Function to conduct fine tuning
    Inputs:
        args - user defined arguments
    """
    # Create dataset
    train_X, train_y = create_dataset(args.train_data_path, args.model_name)
    val_X, val_y = create_dataset(args.val_data_path, args.model_name)
    test_X, test_y = create_dataset(args.test_data_path, args.model_name)
    train_dataset = MultuDataset(train_X, train_y)
    val_dataset = MultuDataset(val_X, val_y)
    test_dataset = MultuDataset(test_X, test_y)

    # Create dataloader
    train_loader = data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True, pin_memory=True, num_workers=args.num_workers)
    val_loader = data.DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, drop_last=False, num_workers=args.num_workers)
    test_loader = data.DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, drop_last=False, num_workers=args.num_workers)

    # Create a PyTorch Lightning trainer with the generation callback
    trainer = pl.Trainer(default_root_dir=os.path.join(args.checkpoint_path, args.model_name + "_model"),
                         accelerator="gpu" if str(args.device).startswith("cuda") else "cpu",
                         devices=1,
                         max_epochs=args.max_epochs,
                         callbacks=[ModelCheckpoint(save_weights_only=True, mode="max", monitor="val_acc"),
                                    LearningRateMonitor("epoch")],
                         enable_progress_bar=True)
    trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard

    # Check whether pretrained model exists. If yes, load it and skip training
    pretrained_filename = os.path.join(args.checkpoint_path, "_" + args.model_name + ".ckpt")
    if os.path.isfile(pretrained_filename):
        print(f"Found pretrained model at {pretrained_filename}, loading...")
        model = Multu_Module.load_from_checkpoint(pretrained_filename) # Automatically loads the model with the saved hyperparameters
    else:
        pl.seed_everything(42) # To be reproducable
        model = Multu_Module(args)
        trainer.fit(model, train_loader, val_loader)
        model = Multu_Module.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training

    # Test best model on validation and test set
    val_result = trainer.test(model, val_loader, verbose=False)
    test_result = trainer.test(model, test_loader, verbose=False)
    result = {"test": test_result[0]["test_acc"], "val": val_result[0]["test_acc"]}

    return model, result

def parseArgs():
    """
    Function to parse user input argument
    """
    parser = argparse.ArgumentParser()

    ## Required parameters
    parser.add_argument("--model_name", default='bert-base-uncased', type=str, required=False,
                        help="name of pre-trained-language model")
    parser.add_argument("--num_classes", default=4, type=int, required=False,
                        help="number of classes")
    parser.add_argument("--train_data_path", default='/Users/marten/UvA labs/DL4NLP_mutual/data/mutual/train/train_1.txt', type=str, required=False,
                        help="path of training data")
    parser.add_argument("--val_data_path", default='/Users/marten/UvA labs/DL4NLP_mutual/data/mutual/dev/dev_1.txt', type=str, required=False,
                        help="path of validation data")
    parser.add_argument("--test_data_path", default='/Users/marten/UvA labs/DL4NLP_mutual/data/mutual/dev/dev_2.txt', type=str, required=False,
                        help="path of test data")
    parser.add_argument("--batch_size", default=8, type=int,
                        help="Batch size per GPU/CPU for evaluation.")
    parser.add_argument("--num_workers", default=4, type=int,
                        help="number of cpus/gpus to run on parallel")
    parser.add_argument("--checkpoint_path", default='', type=str, required=False,
                        help="path to store checkpoints")
    parser.add_argument("--max_epochs", default=1, type=int, required=False,
                        help="Maximum number of epochs, most likely the number of epochs")
    parser.add_argument("--device", default='cpu', type=str, required=False,
                        help="Device you want to train on")
    parser.add_argument("--freeze_lm", default=True, type=bool, required=False,
                        help="If we want to freeze all layers of the model or not")
    parser.add_argument("--optimizer", default='Adam', type=str, required=False,
                        help="Name of the optimizer, Adam, SGD")

    args = parser.parse_args()
    return args

def main():
  args = parseArgs()
  print(args)
  model, result = fine_tune(args)

if __name__ == "__main__":
    main()

